{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weston/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from english_words import english_words_set\n",
    "\n",
    "from skimage import io, color, filters\n",
    "from skimage.transform import resize, rotate\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlst = [\"lions\", 'tigers', \"bears\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e65e6e92158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmess_up_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-e65e6e92158b>\u001b[0m in \u001b[0;36mmess_up_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmess_up_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0malphalst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mchar_to_insert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphalst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrandomidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mess_up_word(word):\n",
    "    alphalst = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n",
    "    char_to_insert = random.choice(list(alphalst))\n",
    "    length = len(word)\n",
    "    randomidx = random.randint(0,length-1)\n",
    "    rmchar = word[randomidx]\n",
    "    newcharlist = []\n",
    "    for idx, char in enumerate(word):\n",
    "        if idx == randomidx:\n",
    "            newcharlist.append(char_to_insert)\n",
    "        else:\n",
    "            newcharlist.append(word[idx])\n",
    "    return \"\".join(newcharlist)\n",
    "\n",
    "            \n",
    "    \n",
    "mess_up_word('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image():\n",
    "    word = np.random.choice(shortlst)\n",
    "    y= word\n",
    "    size = np.random.uniform(20,30)\n",
    "    xpos = np.random.uniform(0,1.2)\n",
    "    ypos = np.random.uniform(0,1.2)\n",
    "    rot = np.random.uniform(-30,30)\n",
    "    word = mess_up_word(word)\n",
    "    \n",
    "    plt.text(1,1, word, size = size, rotation = 0)\n",
    "    plt.xlim([0,2])\n",
    "    plt.ylim([0,2])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'data/{y}.jpg')\n",
    "    plt.close()\n",
    "    X = np.array(Image.open(f'data/{y}.jpg'))\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = [] \n",
    "for wrd in range(1000): \n",
    "    img, lab = make_image()\n",
    "    images.append(img)\n",
    "    labels.append(lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "new_labels = lb.fit_transform(labels)\n",
    "lb.classes_\n",
    "new_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X = np.array(images)\n",
    "y = np.array(new_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(input_shape, nb_classes, dropout, nb_filters, kernel_size,  pool_size, activation='relu',optimizer='adam'):    \n",
    "    model = Sequential()  # model is a linear stack of layers (don't change)\n",
    "\n",
    "    # note: the convolutional layers and dense layers require an activation function\n",
    "    # see https://keras.io/activations/\n",
    "    # and https://en.wikipedia.org/wiki/Activation_function\n",
    "    # options: 'linear', 'sigmoid', 'tanh', 'relu', 'softplus', 'softsign'\n",
    "\n",
    "    model.add(Conv2D(nb_filters,\n",
    "                     (kernel_size[0], kernel_size[1]),\n",
    "                     padding='valid',\n",
    "                     input_shape=input_shape))  # first conv. layer  KEEP\n",
    "    model.add(Activation(activation))  # Activation specification necessary for Conv2D and Dense layers\n",
    "\n",
    "    model.add(Conv2D(nb_filters,\n",
    "                     (kernel_size[0], kernel_size[1]),\n",
    "                     padding='valid'))  # 2nd conv. layer KEEP\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))  # decreases size, helps prevent overfitting\n",
    "    model.add(Dropout(dropout))  # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Flatten())  # necessary to flatten before going into conventional dense layer  KEEP\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "\n",
    "    # now start a typical neural network\n",
    "    model.add(Dense(32))  # (only) 32 neurons in this layer, really?   KEEP\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dropout(dropout))  # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Dense(nb_classes))  # 10 final nodes (one for each class)  KEEP\n",
    "    model.add(Activation('softmax'))  # softmax at end to pick between classes 0-9 KEEP\n",
    "\n",
    "    # many optimizers available, see https://keras.io/optimizers/#usage-of-optimizers\n",
    "    # suggest you KEEP loss at 'categorical_crossentropy' for this multiclass problem,\n",
    "    # and KEEP metrics at 'accuracy'\n",
    "    # suggest limiting optimizers to one of these: 'adam', 'adadelta', 'sgd'\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model flattened out to  (None, 150165)\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 37s 731ms/step - loss: 453.7380 - accuracy: 0.4229 - val_loss: 0.7710 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 37s 748ms/step - loss: 0.7308 - accuracy: 0.6394 - val_loss: 0.7027 - val_accuracy: 0.5900\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 38s 769ms/step - loss: 0.6864 - accuracy: 0.6519 - val_loss: 0.6211 - val_accuracy: 0.7150\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 39s 775ms/step - loss: 0.5842 - accuracy: 0.7615 - val_loss: 0.3601 - val_accuracy: 0.9450\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 39s 771ms/step - loss: 0.4179 - accuracy: 0.8722 - val_loss: 0.3193 - val_accuracy: 0.9050\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 0.3951 - accuracy: 0.8292 - val_loss: 0.2828 - val_accuracy: 0.9250\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 39s 771ms/step - loss: 0.3839 - accuracy: 0.8908 - val_loss: 0.2370 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 39s 776ms/step - loss: 0.3215 - accuracy: 0.9128 - val_loss: 0.2013 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 38s 768ms/step - loss: 0.3265 - accuracy: 0.8794 - val_loss: 0.2046 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 39s 776ms/step - loss: 0.2714 - accuracy: 0.9478 - val_loss: 0.1527 - val_accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# important inputs to the model: don't changes the ones marked KEEP in the functions above\n",
    "batch_size = 16 # number of training samples used at a time to update the weights\n",
    "nb_epoch = 10      # number of passes through the entire train dataset before weights \"final\"\n",
    "nb_filters = 5    # number of convolutional filters to use\n",
    "pool_size = (2,2)  # pooling decreases image size, reduces computation, adds translational invariance\n",
    "kernel_size = (4, 4)  # convolutional kernel size, slides over image to learn features\n",
    "dropout = 0.1\n",
    "activation='relu'\n",
    "optimizer='adam'\n",
    "img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "input_shape = (img_rows,img_cols,3)\n",
    "nb_classes = len(shortlst)\n",
    "\n",
    "model = define_model(input_shape, nb_classes, dropout, nb_filters, kernel_size, pool_size)\n",
    "\n",
    "# during fit process watch train and test error simultaneously\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2d780e36333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10000):\n",
    "#     images = []\n",
    "#     labels = [] \n",
    "#     for wrd in range(5000): \n",
    "#         img, lab = make_image()\n",
    "#         images.append(img)\n",
    "#         labels.append(lab)\n",
    "#     lb = LabelBinarizer()\n",
    "#     new_labels = lb.fit_transform(labels)\n",
    "#     X = np.array(images)\n",
    "#     y = np.array(new_labels)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#     history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "#     score = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     scorelst.append(score)\n",
    "#     print(score)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# scorelst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,  verbose=1, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
