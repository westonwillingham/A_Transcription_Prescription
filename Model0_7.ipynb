{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weston/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from english_words import english_words_set\n",
    "\n",
    "from skimage import io, color, filters\n",
    "from skimage.transform import resize, rotate\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlst = [\"lions\", 'tigers', \"bears\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image():\n",
    "    word = np.random.choice(shortlst)\n",
    "    size = np.random.uniform(20,70)\n",
    "    xpos = np.random.uniform(0,1.2)\n",
    "    ypos = np.random.uniform(0,1.2)\n",
    "    rot = np.random.uniform(-30,30)\n",
    "    y = word\n",
    "    #print(y)\n",
    "    plt.text(xpos,ypos, y, size = 30, rotation = 0)\n",
    "    plt.xlim([0,2])\n",
    "    plt.ylim([0,2])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'data/{y}.jpg')\n",
    "    plt.close()\n",
    "    X = np.array(Image.open(f'data/{y}.jpg'))\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = [] \n",
    "for wrd in range(1000): \n",
    "    img, lab = make_image()\n",
    "    images.append(img)\n",
    "    labels.append(lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "new_labels = lb.fit_transform(labels)\n",
    "lb.classes_\n",
    "new_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X = np.array(images)\n",
    "y = np.array(new_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(input_shape, nb_classes, dropout, nb_filters, kernel_size,  pool_size, activation='relu',optimizer='adam'):    \n",
    "    model = Sequential()  # model is a linear stack of layers (don't change)\n",
    "\n",
    "    # note: the convolutional layers and dense layers require an activation function\n",
    "    # see https://keras.io/activations/\n",
    "    # and https://en.wikipedia.org/wiki/Activation_function\n",
    "    # options: 'linear', 'sigmoid', 'tanh', 'relu', 'softplus', 'softsign'\n",
    "\n",
    "    model.add(Conv2D(nb_filters,\n",
    "                     (kernel_size[0], kernel_size[1]),\n",
    "                     padding='valid',\n",
    "                     input_shape=input_shape))  # first conv. layer  KEEP\n",
    "    model.add(Activation(activation))  # Activation specification necessary for Conv2D and Dense layers\n",
    "\n",
    "    model.add(Conv2D(nb_filters,\n",
    "                     (kernel_size[0], kernel_size[1]),\n",
    "                     padding='valid'))  # 2nd conv. layer KEEP\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))  # decreases size, helps prevent overfitting\n",
    "    model.add(Dropout(dropout))  # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Flatten())  # necessary to flatten before going into conventional dense layer  KEEP\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "\n",
    "    # now start a typical neural network\n",
    "    model.add(Dense(32))  # (only) 32 neurons in this layer, really?   KEEP\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dropout(dropout))  # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Dense(nb_classes))  # 10 final nodes (one for each class)  KEEP\n",
    "    model.add(Activation('softmax'))  # softmax at end to pick between classes 0-9 KEEP\n",
    "\n",
    "    # many optimizers available, see https://keras.io/optimizers/#usage-of-optimizers\n",
    "    # suggest you KEEP loss at 'categorical_crossentropy' for this multiclass problem,\n",
    "    # and KEEP metrics at 'accuracy'\n",
    "    # suggest limiting optimizers to one of these: 'adam', 'adadelta', 'sgd'\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model flattened out to  (None, 150165)\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 96s 2s/step - loss: 677.9557 - accuracy: 0.3735 - val_loss: 5.1325 - val_accuracy: 0.2850\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 65s 1s/step - loss: 2.3793 - accuracy: 0.5997 - val_loss: 2.6225 - val_accuracy: 0.3350\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 67s 1s/step - loss: 0.8344 - accuracy: 0.7208 - val_loss: 2.1372 - val_accuracy: 0.3250\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 72s 1s/step - loss: 0.4342 - accuracy: 0.7964 - val_loss: 2.6383 - val_accuracy: 0.3000\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 71s 1s/step - loss: 0.3932 - accuracy: 0.8264 - val_loss: 3.0085 - val_accuracy: 0.3050\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 80s 2s/step - loss: 0.3161 - accuracy: 0.8493 - val_loss: 2.7405 - val_accuracy: 0.3050\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 81s 2s/step - loss: 0.3065 - accuracy: 0.8389 - val_loss: 2.7645 - val_accuracy: 0.3350\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 81s 2s/step - loss: 0.2787 - accuracy: 0.8553 - val_loss: 2.6339 - val_accuracy: 0.3250\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 65s 1s/step - loss: 0.2788 - accuracy: 0.8739 - val_loss: 2.3167 - val_accuracy: 0.3300\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 63s 1s/step - loss: 0.2386 - accuracy: 0.9180 - val_loss: 2.3493 - val_accuracy: 0.3400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# important inputs to the model: don't changes the ones marked KEEP in the functions above\n",
    "batch_size = 16 # number of training samples used at a time to update the weights\n",
    "nb_epoch = 10      # number of passes through the entire train dataset before weights \"final\"\n",
    "nb_filters = 5    # number of convolutional filters to use\n",
    "pool_size = (2,2)  # pooling decreases image size, reduces computation, adds translational invariance\n",
    "kernel_size = (4, 4)  # convolutional kernel size, slides over image to learn features\n",
    "dropout = 0.1\n",
    "activation='relu'\n",
    "optimizer='adam'\n",
    "img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "input_shape = (img_rows,img_cols,3)\n",
    "nb_classes = len(shortlst)\n",
    "\n",
    "model = define_model(input_shape, nb_classes, dropout, nb_filters, kernel_size, pool_size)\n",
    "\n",
    "# during fit process watch train and test error simultaneously\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3493282794952393, 0.3400000035762787]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10000):\n",
    "#     images = []\n",
    "#     labels = [] \n",
    "#     for wrd in range(5000): \n",
    "#         img, lab = make_image()\n",
    "#         images.append(img)\n",
    "#         labels.append(lab)\n",
    "#     lb = LabelBinarizer()\n",
    "#     new_labels = lb.fit_transform(labels)\n",
    "#     X = np.array(images)\n",
    "#     y = np.array(new_labels)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#     history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "#     score = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     scorelst.append(score)\n",
    "#     print(score)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# scorelst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,  verbose=1, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
